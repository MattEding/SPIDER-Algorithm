{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection of imbalanced datasets.\n",
      "\n",
      "This collection of datasets has been proposed in [1]_. The\n",
      "characteristics of the available datasets are presented in the table\n",
      "below.\n",
      "\n",
      " ID    Name           Repository & Target           Ratio  #S       #F\n",
      " 1     ecoli          UCI, target: imU              8.6:1  336      7\n",
      " 2     optical_digits UCI, target: 8                9.1:1  5,620    64\n",
      " 3     satimage       UCI, target: 4                9.3:1  6,435    36\n",
      " 4     pen_digits     UCI, target: 5                9.4:1  10,992   16\n",
      " 5     abalone        UCI, target: 7                9.7:1  4,177    10\n",
      " 6     sick_euthyroid UCI, target: sick euthyroid   9.8:1  3,163    42\n",
      " 7     spectrometer   UCI, target: >=44             11:1   531      93\n",
      " 8     car_eval_34    UCI, target: good, v good     12:1   1,728    21\n",
      " 9     isolet         UCI, target: A, B             12:1   7,797    617\n",
      " 10    us_crime       UCI, target: >0.65            12:1   1,994    100\n",
      " 11    yeast_ml8      LIBSVM, target: 8             13:1   2,417    103\n",
      " 12    scene          LIBSVM, target: >one label    13:1   2,407    294\n",
      " 13    libras_move    UCI, target: 1                14:1   360      90\n",
      " 14    thyroid_sick   UCI, target: sick             15:1   3,772    52\n",
      " 15    coil_2000      KDD, CoIL, target: minority   16:1   9,822    85\n",
      " 16    arrhythmia     UCI, target: 06               17:1   452      278\n",
      " 17    solar_flare_m0 UCI, target: M->0             19:1   1,389    32\n",
      " 18    oil            UCI, target: minority         22:1   937      49\n",
      " 19    car_eval_4     UCI, target: vgood            26:1   1,728    21\n",
      " 20    wine_quality   UCI, wine, target: <=4        26:1   4,898    11\n",
      " 21    letter_img     UCI, target: Z                26:1   20,000   16\n",
      " 22    yeast_me2      UCI, target: ME2              28:1   1,484    8\n",
      " 23    webpage        LIBSVM, w7a, target: minority 33:1   34,780   300\n",
      " 24    ozone_level    UCI, ozone, data              34:1   2,536    72\n",
      " 25    mammography    UCI, target: minority         42:1   11,183   6\n",
      " 26    protein_homo   KDD CUP 2004, minority        111:1  145,751  74\n",
      " 27    abalone_19     UCI, target: 19               130:1  4,177    10\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Ding, Zejin, \"Diversified Ensemble Classifiers for Highly\n",
      "   Imbalanced Data Learning and their Application in Bioinformatics.\"\n",
      "   Dissertation, Georgia State University, (2011).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import imblearn.datasets._zenodo as zenodo\n",
    "print(zenodo.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.datasets import fetch_datasets\n",
    "\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, make_scorer\n",
    "from imblearn.metrics import specificity_score, geometric_mean_score\n",
    "\n",
    "# samplers\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SPIDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(X, y, sampler=None):\n",
    "    time_list = []\n",
    "    size_list = []\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    specificity_list = []\n",
    "    geometric_mean_list = []\n",
    "    roc_auc_list = []\n",
    "    \n",
    "    logreg = LogisticRegression(solver='lbfgs', random_state=0, n_jobs=-1)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state=0)\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        \n",
    "        t0 = time()\n",
    "        if sampler:\n",
    "            X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "        else:\n",
    "            X_resampled, y_resampled = X_train, y_train\n",
    "        t1 = time()\n",
    "        time_list.append(t1 - t0)\n",
    "        size_list.append(len(y_resampled))\n",
    "\n",
    "        logreg.fit(X_resampled, y_resampled)\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        y_score = logreg.decision_function(X_test)\n",
    "\n",
    "        accuracy_list.append(accuracy_score(y_test, y_pred))\n",
    "        precision_list.append(precision_score(y_test, y_pred))\n",
    "        recall_list.append(recall_score(y_test, y_pred))\n",
    "        specificity_list.append(specificity_score(y_test, y_pred))\n",
    "        geometric_mean_list.append(geometric_mean_score(y_test, y_pred))\n",
    "        roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    return dict(\n",
    "        accuracy=np.mean(accuracy_list),\n",
    "        precision=np.mean(precision_list),\n",
    "        recall=np.mean(recall_list),\n",
    "        specificity=np.mean(specificity_list),\n",
    "        geometric_mean=np.mean(geometric_mean_list),\n",
    "        roc_auc=np.mean(roc_auc_list),\n",
    "        size=np.mean(size_list),\n",
    "        time=np.mean(time_list),\n",
    "    )\n",
    "\n",
    "\n",
    "def benchmark(id):\n",
    "    name = list(fetch_datasets())[id-1]\n",
    "    dataset = fetch_datasets()[name]\n",
    "    \n",
    "    X, y = dataset.data, dataset.target\n",
    "\n",
    "    ncr = NeighbourhoodCleaningRule(random_state=0, n_jobs=-1)\n",
    "    smote = SMOTE(random_state=0, n_jobs=-1)\n",
    "    weak = SPIDER(kind='weak', n_jobs=-1)\n",
    "    relabel = SPIDER(kind='relabel', n_jobs=-1)\n",
    "    strong = SPIDER(kind='strong', n_jobs=-1)\n",
    "\n",
    "    results_none = pipeline(X, y)\n",
    "    results_smote = pipeline(X, y, smote)\n",
    "    results_ncr = pipeline(X, y, ncr)\n",
    "    results_weak = pipeline(X, y, weak)\n",
    "    results_relabel = pipeline(X, y, relabel)\n",
    "    results_strong = pipeline(X, y, strong)\n",
    "\n",
    "    results_list =  [results_none, results_smote, results_ncr, results_weak, results_relabel, results_strong]\n",
    "\n",
    "    accuracies =    [r['accuracy'] for r in results_list]\n",
    "    precisions =    [r['precision'] for r in results_list]\n",
    "    recalls =       [r['recall'] for r in results_list]\n",
    "    specificities = [r['specificity'] for r in results_list]\n",
    "    geometric_means=[r['geometric_mean'] for r in results_list]\n",
    "    roc_aucs =      [r['roc_auc'] for r in results_list]\n",
    "    sizes =         [r['size'] for r in results_list]\n",
    "    times =         [r['time'] for r in results_list]\n",
    "\n",
    "    results_dict = dict(\n",
    "        accuracy=accuracies,\n",
    "        precision=precisions,\n",
    "        recall=recalls,\n",
    "        specificity=specificities,\n",
    "        geometric_mean=geometric_means,\n",
    "        roc_auc=roc_aucs,\n",
    "        size=sizes,\n",
    "        time=times,\n",
    "    )\n",
    "\n",
    "    index = pd.Index(['none', 'smote', 'ncr','weak', 'relabel', 'strong'], name=name)\n",
    "    results_df = pd.DataFrame(results_dict, index=index)\n",
    "\n",
    "    results_df.to_pickle(f'../benchmark_{name}.pkl')\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in np.arange(27) + 1:\n",
    "    if id == 26:\n",
    "        continue\n",
    "    benchmark(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_benchmark(df, save=True):\n",
    "    palette = sns.diverging_palette(255, 133, l=60, n=6, center=\"dark\")\n",
    "    # palette = 'inferno'\n",
    "    sns.set_style('darkgrid')\n",
    "    sns.set_palette(palette)\n",
    "    title = \" \".join(df.index.name.split(\"_\")).title()\n",
    "    name = df.index.name\n",
    "    df.index.name = None\n",
    "    fig = plt.figure(dpi=140)\n",
    "    df.loc[methods, metrics].T.plot.bar(figsize=(18, 6), title=title, rot=0, ax=plt.gca())\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Score')\n",
    "    if save:\n",
    "        plt.savefig(f'../benchmark_{name}.png', bbox_inches='tight')\n",
    "\n",
    "methods = ['none', 'smote', 'ncr', 'weak', 'relabel', 'strong']\n",
    "metrics = ['accuracy', 'precision', 'recall', 'specificity', 'geometric_mean']\n",
    "\n",
    "# save graphs\n",
    "cwd = Path.cwd()\n",
    "benchmarks = [pkl for pkl in cwd.parent.iterdir() if 'benchmark' in pkl.stem and pkl.suffix == '.pkl']\n",
    "for pkl in benchmarks:\n",
    "    df = pd.read_pickle(pkl)\n",
    "    plot_benchmark(df, save=True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save CSVs\n",
    "for pkl in benchmarks:\n",
    "    df = pd.read_pickle(pkl)\n",
    "    name = df.index.name\n",
    "    df.to_csv(f'../benchmark_{name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
